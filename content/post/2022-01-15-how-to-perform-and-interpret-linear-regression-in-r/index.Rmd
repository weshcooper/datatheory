---
title: Linear Regression in R on Boston Housing Data
author: Wes H Cooper
date: '2022-01-16'
slug: []
categories: []
draft: false
tags:
  - simple linear regession
  - multiple linear regression
description: In this post, we will cover simple linear regression and multiple linear regression, popular statistical techniques used to analyze data when the dependent variable is continuous. To explain how to do this analysis in R we will examine a data set of housing data in Boston. The data set contains 506 census tracts of Boston from the 1970 census.
cover: img/michael-browning-ZLN2WOVpjCo-unsplash.jpg
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)

```
## Introduction

In this post, we will cover simple linear regression and multiple linear regression, popular statistical techniques used to analyze data when the dependent variable is continuous. To explain how to do this analysis in R we will examine a data set of housing data in Boston. The data set contains 506 census tracts of Boston from the 1970 census. The variables in the data set can be used to predict `cmedv`, the median value of owner-occupied homes in 1000's of USD.

## Examining the Data 

After loading the data set, calling `head()` on the data set produces the following:

```{r Load-Data-Head}
library(mlbench)
data("BostonHousing2")
head(BostonHousing2)

```

From `head()` we can see the 19 columns. For this project, I'm most interested in examining the relationship between `rm` and `cmedv`. `rm` tells us how many rooms are in a house while `cmedv` tells us the median value of owner-occupied homes in 1000's of USD. I'm thinking that the amount of rooms in a house may have a relationship with the cost of the house.

## Testing Correlation

To test the correlation between variables a correlation coefficient can be calculated, where `BostonHousing2$rm` is the independent variable and `BostonHousing2$cmedv` is the dependent variable.

```{r CorrelationTest}

cor.test(BostonHousing2$rm, BostonHousing2$cmedv)

```

As can be seen from the output there is a moderately strong correlation between 'rm' and 'cmedv' with a correlation coefficient of 0.696 and a statistically significant p-value of less than 0.05.

## Creating a Scatterplot

We can also create a scatter plot with the function `plot()` to examine if it looks like there is a linear relationship. 

```{r CreatePlot}
# Create Plot
plot(x = BostonHousing2$rm, y = BostonHousing2$cmedv, 
     main = "Bedrooms and Housing Prices", xlab = "Bedrooms", ylab = "Prices")
```

From examining the plot, it looks like there's a slightly positive correlation.


## Running Linear Regression Model

We can now run the linear regression. A linear regression in r can be performed by using the `lm()` function. The format for `lm()` is as follows:

`lm([dependent variable] ~ [independent variables], data = [data frame])`

In the following example, a linear regression is performed on the Boston Housing data set. As mentioned earlier, we are looking at the relationship between `cmedv` and `rm` where `cmedv` is the median value of owner-occupied homes in 1000's of USD and `rm` is the number of rooms in a house.

```{r LinearModel}
# Fit linear model
model = lm(cmedv ~ rm, BostonHousing2)

# Examine model
model
summary(model)
```

## Interpreting Linear Regression Output

From the output it can be seen that the p-value for `rm` is statistically significant with a value less than 0.05. However, we also need to take a look at the rest of the output. We'll look at the Multiple R Squared next. 

The Multiple R squared indicates how well the fit is between the model or regression line and the data. In particular, it explains the proportion of variance in the dependent variable explained by the independent variable. In our example, about 48% of the of the variation in `cmedv` can be explained by `rm`, meaning that 52% of the variance can be explained by other variables. This may mean that it may be better to create a model based on a multiple linear regression, but for now we'll continue with the simple linear regression.

We can ignore the adjusted R squared for now because it is only used in multiple linear regression (when there is more than one independent variable). Both the adjusted R squared and multiple R squared would likely be higher when taking into consideration other independent variables.

## Examinng the Residuals

Next we can examine the residuals to better see how the model fits the data.

```{r Residuals}
plot(model$residuals)
```

From the plot we can see a fairly clear pattern in the data. This means that the model could be improved upon, likely because the model is either missing variables or the independent variable needs to be transformed. If the model had a better fit then there would be no pattern in the residuals. We'll explore the topic of improving the model in future blog posts.

